{
  
    
        "post0": {
            "title": "Plotting LEGO Data with Python and Pandas",
            "content": "When the first wave of COVID-19 hit Germany in spring 2020 and everyone started their lockdown hobby, I decided to start sorting my LEGO bricks. Little did I know that I opened up Pandora&#39;s box with that. To make it short: this project is not finished yet and by now I doubt that it ever will. . While sorting through the last decades of LEGO, I quickly recognized some trends. For example, most the iconic 2x4 bricks that everyone associates with LEGO were very old. It seems that this classic brick is even less popular than I thought beforehands. On the other hand I was astonished, how many 1x2 plates I obtained through the years. . A quick Google search revealed that Rebrickable offers a huge LEGO database for download at https://rebrickable.com/downloads/. Because I was interested in data analysis with Python, I grabbed the opportunity and tried to find out, whether or not I could create some interesting plots about the evolution of LEGO bricks. Here, I want to share some first results, when I looked into the brick colors. . Preparing the Data Set . Let&#39;s start with importing the libraries. In the next step I am loading the csv files (that I downloaded from Rebrickable and saved in a folder data) with panda&#39;s read_csv() function. . import pandas as pd import seaborn as sns import matplotlib.pyplot as plt colors = pd.read_csv(&#39;data/colors.csv&#39;) inventories = pd.read_csv(&#39;data/inventories.csv&#39;) inventory_parts = pd.read_csv(&#39;data/inventory_parts.csv&#39;) sets = pd.read_csv(&#39;data/sets.csv&#39;) . Next, I merge the different DataFrames into a single DataFrame. This DataFrame contains information about all bricks since 1949. . df = pd.merge(inventories, sets, how=&#39;left&#39;, left_on=[&#39;set_num&#39;], right_on=[&#39;set_num&#39;]) df = pd.merge(df, inventory_parts, how=&#39;left&#39;, left_on=[&#39;id&#39;], right_on=[&#39;inventory_id&#39;]) df = pd.merge(df, colors, how=&#39;left&#39;, left_on=[&#39;color_id&#39;], right_on=[&#39;id&#39;]) . .describe() is a handy function to get quick statistics on the numeric columns of the DataFrame. You can see that the first year (min of year) is indeed the year of 1949. . Something interesting that strucked me here is the max of num_parts, so the maximum number of parts in a set. The database contains a set with 9,987 bricks! However, the LEGO set with the highest number of bricks as of today is the 10276 Colloseum with only 9,036 bricks. . df.describe() . id_x version year theme_id num_parts inventory_id color_id quantity id_y . count 885528.00000 | 885528.000000 | 835788.000000 | 835788.000000 | 835788.000000 | 883210.000000 | 883210.000000 | 883210.000000 | 883210.000000 | . mean 21853.00518 | 1.063634 | 2008.949127 | 353.415401 | 620.526943 | 21820.226303 | 106.676630 | 3.280416 | 106.676630 | . std 21045.90867 | 0.295775 | 11.209785 | 228.058466 | 901.623679 | 21011.268548 | 766.123959 | 8.812455 | 766.123959 | . min 1.00000 | 1.000000 | 1949.000000 | 1.000000 | 0.000000 | 1.000000 | -1.000000 | 1.000000 | -1.000000 | . 25% 7040.00000 | 1.000000 | 2004.000000 | 155.000000 | 146.000000 | 7037.000000 | 3.000000 | 1.000000 | 3.000000 | . 50% 13903.00000 | 1.000000 | 2013.000000 | 389.000000 | 362.000000 | 13901.000000 | 15.000000 | 2.000000 | 15.000000 | . 75% 30014.00000 | 1.000000 | 2017.000000 | 576.000000 | 733.000000 | 29994.000000 | 71.000000 | 3.000000 | 71.000000 | . max 89176.00000 | 8.000000 | 2021.000000 | 712.000000 | 9987.000000 | 89176.000000 | 9999.000000 | 1440.000000 | 9999.000000 | . Let&#39;s investigate this mysterious set with 9,987 parts/bricks. It turns out this was a huge set from the Chima theme that was never available to retail: https://rebrickable.com/sets/BIGBOX-1/the-ultimate-battle-for-chima/ . df.loc[df[&#39;num_parts&#39;].idxmax()] . id_x 30510 version 1 set_num BIGBOX-1 name_x The Ultimate Battle for Chima year 2015 theme_id 571 num_parts 9987 inventory_id NaN part_num NaN color_id NaN quantity NaN is_spare NaN id_y NaN name_y NaN rgb NaN is_trans NaN Name: 677275, dtype: object . After this surprising finding, let&#39;s move on with only the columns that we need for now. Remember that every row contains a single brick and the names artifacts from our DataFrame merge in the beginning: . year: release year the set, in which the brick was used | set_num: number of the set, in which the brick was used | name_y: name of the brick&#39;s color | rgb: RGB code of the color (in hexadecimal) | is_trans: whether the color transparent or not | . I keep only these columns (by copying and overwriting the DataFrame). Also, I remove this year&#39;s data, because I only care for full years. . df = df[[&#39;year&#39;, &#39;set_num&#39;, &#39;name_y&#39;, &#39;rgb&#39;, &#39;is_trans&#39;]].copy() # Remove 2021 from df df = df[df.year != 2021] df.head() . year set_num name_y rgb is_trans . 0 2004.0 | 7922-1 | Dark Bluish Gray | 6C6E68 | f | . 1 2004.0 | 7922-1 | Light Gray | 9BA19D | f | . 2 2004.0 | 7922-1 | Black | 05131D | f | . 3 2004.0 | 7922-1 | Orange | FE8A18 | f | . 4 2012.0 | 3931-1 | Trans-Clear | FCFCFC | t | . Plotting the Decline of the Classic Colors . The first thing I am looking at is the distribution of the classic colors (red, blue, yellow) over time. These colors are rarely used in todays sets and it is no surprise that most of these colors in my collection are from sets released in the 70s and 80s. . I am a fan of Nate Silver&#39;s politics website FiveThirtyEight (https://fivethirtyeight.com/) and their concise plots. Python&#39;s Matplotlib library has a built-in plot style that resembles FiveThirtyEight&#39;s plots, which is why I am going to use this style for my plots. However, some tweaking is needed to make these plots more similar to the real FiveThirtyEight plots. . df_colors = df.groupby([&#39;year&#39;, &#39;name_y&#39;, &#39;rgb&#39;]).size().reset_index(name=&#39;counts&#39;) df_colors = df_colors.groupby([&#39;year&#39;, &#39;name_y&#39;, &#39;rgb&#39;]).agg({&#39;counts&#39;: &#39;sum&#39;}) # Calculate the relative percentages df_colors = df_colors.groupby(level=0).apply(lambda x: 100 * x / float(x.sum())).reset_index() df_colors.columns = [&#39;Year&#39;, &#39;Color&#39;, &#39;RGB&#39;, &#39;Percentage&#39;] # Filter for red, blue and yellow df_rby = df_colors[df_colors.Color.eq(&#39;Blue&#39;) | df_colors.Color.eq(&#39;Red&#39;) | df_colors.Color.eq(&#39;Yellow&#39;)] # Fill the color palette based on the actual RGB values cpalette = sns.color_palette(&#39;#&#39; + df_rby.RGB.unique()) # Create figure and set plot style plt.style.use(&#39;fivethirtyeight&#39;) plt.figure(figsize=(10,6)) # Plot with Seaborn (and distinguish with hue=&#39;Color&#39;) ax = sns.lineplot(x=&#39;Year&#39;, y=&#39;Percentage&#39;, hue=&#39;Color&#39;, palette=cpalette, data=df_rby, legend=False) # Set the axis limits plt.ylim(-1, 46) plt.xlim(xmax=2021) # Create bold line at y=0 ax.axhline(y=0, color=&#39;#414141&#39;, linewidth=1.3, alpha=0.5) # Set font size for the axis tick labels and specify y tick labels... ax.tick_params(axis = &#39;both&#39;, which = &#39;major&#39;, labelsize = 14) ax.set_yticklabels(labels = [&#39;&#39;,&#39;0 &#39;, &#39;10 &#39;, &#39;20 &#39;, &#39;30 &#39;, &#39;40%&#39;]) # ... but hide the actual axis labels ax.xaxis.label.set_visible(False) ax.yaxis.label.set_visible(False) # Make the title and subtitle ax.text(x = 1940, y = 51, s = &quot;The decline of the classic colors&quot;, fontsize = 19.5, weight = &#39;bold&#39;, alpha = .75) ax.text(x = 1940, y = 48, s = &#39;Distribution of blue, red and yellow bricks in all LEGO sets throughout the decades&#39;, fontsize = 16.5, alpha = .85) # Make the footer ax.text(x = 1945, y = -8, s = &#39;©Nicolas P. Winkler&#39;, fontsize = 14, color = &#39;grey&#39;, alpha=0.7); ax.text(x = 2000, y = -8, s = &#39;Source: Rebrickable.com &#39;, fontsize = 14, color = &#39;grey&#39;, alpha=0.7); # Show plot plt.show() . Plotting the Rise of New Colors . Let&#39;s now take a look new colors. For that I only care about colors that were introduced after 1990. The most notable color change certainly happend in 2003, when LEGO replaced some colors with more vibrant versions. Most notably the change from light gray and dark gray to light bluish gray and dark bluish gray. For this example, I only look at the ten colors that were most popular in 2020. . df_1990 = df_colors.loc[df_colors[&quot;Year&quot;]&lt;=1990] df_2020 = df_colors.loc[df_colors[&quot;Year&quot;]==2020] # Sort the DataFrame by percentage df_2020 = df_2020.sort_values(by=[&#39;Percentage&#39;], ascending=False) # Get the ten most popular new colors in 2020 # Filter df_2020 to only show colors that do not appear in df_1990 df_top10_new_colors = df_2020[~df_2020.Color.isin(df_1990.Color)].head(10) # And now filter the DataFrame that contains all bricks to only contain the top10 new colors. df_new_colors = df_colors[df_colors.Color.isin(df_top10_new_colors.Color)] . cpalette = sns.color_palette(&#39;#&#39;+df_top10_new_colors.RGB.unique()) # Get the names of all colors and save in list cnames = df_top10_new_colors.Color.unique() # Create figure and set plot style plt.figure(figsize=(10,6)) plt.style.use(&#39;fivethirtyeight&#39;) # Plot with Seaborn (and distinguish with hue=&#39;Color&#39;) ax = sns.lineplot(x=&#39;Year&#39;, y=&#39;Percentage&#39;, hue=&#39;Color&#39;, palette=cpalette, data=df_new_colors, legend=False) # Set the axis limits plt.xlim(1989, 2021) # Create bold line at y=0 ax.axhline(y=0, color=&#39;#414141&#39;, linewidth=1.3, alpha=0.5) # Set font size for the axis tick labels and specify y tick labels... ax.tick_params(axis = &#39;both&#39;, which = &#39;major&#39;, labelsize = 14) # ... but hide the actual axis labels ax.xaxis.label.set_visible(False) ax.yaxis.label.set_visible(False) # Specify y ticks and labels... ax.set_yticks([0, 5, 10, 15]) ax.set_yticklabels(labels = [&#39;0 &#39;, &#39;5 &#39;, &#39;10 &#39;, &#39;15%&#39;]) # Make the title and subtitle ax.text(x = 1987, y = 17.5, s = &quot;The rise of new colors&quot;, fontsize = 19.5, weight = &#39;bold&#39;, alpha = .75) ax.text(x = 1987, y = 16.5, s = &#39;Distribution of colors that were introduced after 1990&#39;, fontsize = 16.5, alpha = .85) # Make the footer ax.text(x = 1987, y = -3, s = &#39; ©Nicolas P. Winkler&#39;, fontsize = 14, color = &#39;gray&#39;, alpha=0.7); ax.text(x = 2012, y = -3, s = &#39;Source: Rebrickable.com &#39;, fontsize = 14, color = &#39;gray&#39;, alpha=0.7); # Add labels of legend. The labels text is the color name, the labels color is the RGB code. palette = sns.color_palette(&#39;#&#39; + df_top10_new_colors.RGB.unique()) ax.text(x = 1991, y = 14, s = cnames[0], color = palette[0], weight = &#39;bold&#39;) ax.text(x = 1991, y = 13, s = cnames[1], color = palette[1], weight = &#39;bold&#39;) ax.text(x = 1991, y = 12, s = cnames[2], color = palette[2], weight = &#39;bold&#39;) ax.text(x = 1991, y = 11, s = cnames[3], color = palette[3], weight = &#39;bold&#39;) ax.text(x = 1991, y = 10, s = cnames[4], color = palette[4], weight = &#39;bold&#39;) ax.text(x = 1991, y = 9, s = cnames[5], color = palette[5], weight = &#39;bold&#39;) ax.text(x = 1991, y = 8, s = cnames[6], color = palette[6], weight = &#39;bold&#39;) ax.text(x = 1991, y = 7, s = cnames[7], color = palette[7], weight = &#39;bold&#39;) ax.text(x = 1991, y = 6, s = cnames[8], color = palette[8], weight = &#39;bold&#39;) ax.text(x = 1991, y = 5, s = cnames[9], color = palette[9], weight = &#39;bold&#39;) # Show plot plt.show() .",
            "url": "https://niphiwi.github.io/blog/2021/03/06/Plotting-LEGO-Data-with-Python.html",
            "relUrl": "/2021/03/06/Plotting-LEGO-Data-with-Python.html",
            "date": " • Mar 6, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Air Pollution Monitoring With Mobile Robots",
            "content": "Here, I want to briefly present some ideas from my first research paper [1]. My intention is that even my blue-collar parents would get a basic understanding of the research topic, even though they’d have to rely on DeepL.com for translation – sorry! ;-) . . Air Pollution Affects Us All . Thanks to our major car manufacturers and Dieselgate, we all know that air pollution is a major risk. In fact, more than 400,000 premature deaths are linked to air pollution – and that’s only in the European Union! According to the WHO, more than seven million people worldwide are killed by polluted air. Nine out of ten people breathe air that exceeds the WHO’s guidelines. This means that diesel cars are only a small part of the problem. Air pollution has to be tackled in many different areas and I will focus on the area of occupational health. Especially industry workers – hello, dad! – are exposed to toxic gases and dust particles that are harmful to the human body. . For the sake of simplification, let’s imagine a steel factory: . Here we see some workers and also an area of polluted air. Polluted air may consist of dust particles or gases that emerge during industrial processes in the factory. To minimize the impact of air pollution on our workers, we have to reduce the concentration of toxic particles (e.g., by adapting the factory’s ventilation system) or at a minimum reduce the time our workers are exposed to these hazardous spots. The latter would be particularly easy in the example above. . Approaches for Environmental Monitoring . In reality, however, we have the problem that we don’t really know where these highly polluted areas actually are. Since airborne particles are distributed by underlying physical processes (e.g., advection and diffusion), predicting the distribution of air pollutants is no trivial task. Health experts who try to estimate the health risks for workers can typically only take measurements at one location each time. During their measurement campaign, they may cover different locations, but in the end, they have information of varying recency – and for many areas, they don’t have any measurements at all. On top of that, a human health expert cannot monitor the factory 24/7. Typically, the exposition of a worker is estimated based on single-day campaigns. Seasonal effects like weekends or intermediate increased production volume can only partly be taken into account. The takeaway is: Even with expert knowledge it is not always possible to get a precise understanding of the spatial and temporal distribution of air pollutants. What’s the best approach to get a precise understanding of the distribution of air pollutants in a given area? The boring, scientific answer is: we don’t know. But we have some ideas. Our idea is a combinatory system that consists out of stationary sensors and mobile robots. The system is coined RASEM (Robot-assisted Environmental Monitoring for Air Quality Assessment in Industrial Scenarios) after the same-named European funded research project [2]. The hardware system consists of the following two pillars: . Stationary Sensing Nodes . Multiple stationary sensing nodes form a wireless sensor network. A single sensing node consists of sensors for temperature, humidity, dust, and gas. Here, we are using very low-cost sensors to achieve unit costs below 300 € per node. This enables us the continuous monitoring of environmental parameters at different locations. The more nodes we set up, the better is our rough understanding of the overall concentration levels. . Mobile Robots . We are using ground and aerial robots that are equipped with high-quality sensors. With the robots, we can collect data in areas that are not covered by the stationary sensors. We can also cross-calibrate the low-cost sensors with the high-quality sensors. . Reducing Health Risks by Making Sense Out of Data . Getting the data is only one part of the process. The end result should be to minimize the health risks for humans by developing effective mitigation measures. To enable this, we aim to develop algorithms that create 3D distribution maps of air pollutants. These maps shall visualize, how specific air pollutants are distributed in the factory overtime. . Of course, this is already an active research field and different approaches for this problem exist. So-called Gaussian processes are widely used to represent the distribution of pollutants. Researchers have proven that mobile robots, stationary sensor networks, or the combination of both yield good results for this task. Recent research also tries to leverage advances in the field of machine learning. . With our RASEM system, we aim to develop novel algorithms to predict the distribution of dust and gases at unknown locations. One potential that comes into mind is the incorporation of additional information into a model. Most models are primarily data-driven, which means their prediction is mainly based on the immediate input data. In contrast, whenever we humans perform predictions, we make heavy use of our intuition. We just know that we can’t move through walls, because we learned this the hard way a long time ago (an except maybe if you’re delivered a Hogwarts acceptance letter by owl post around your eleventh birthday, but then you’re out of the scope of this blog). Likewise, the performance of a model could be improved significantly, if an intuition on the relevant physics is incorporated into it. Better predictions of the distribution of air pollutants could be made by inputting not only sensor measurements, but also information on industrial processes (e.g., how often and how long a specific industrial process occurs) or the environment (e.g., how the factory looks like and which areas are used for what). . . In the upcoming year, we are going to evaluate the proposed system under real conditions at industrial sites in Finland. With our project, which is a cooperation of diverse research institutes and industrial partners, we aim to generate new knowledge that can eventually contribute a small part to the global risk of air pollution. . References . [1] NP Winkler, PP Neumann, A Säämänen, E Schaffernicht, AJ Lilienthal: High-quality meets low-cost: Approaches for hybrid-mobility sensor networks, Materials Today: Proceedings, Volume 32, Part 2, 2020, Pages 250-253, DOI: https://doi.org/10.1016/j.matpr.2020.05.799. . [2] https://projects.safera.eu/project/20 .",
            "url": "https://niphiwi.github.io/blog/2020/01/31/Air-Pollution-Monitoring-with-Mobile-Robots.html",
            "relUrl": "/2020/01/31/Air-Pollution-Monitoring-with-Mobile-Robots.html",
            "date": " • Jan 31, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ". Contrary to what this blog says, I also have things other than pasta on my mind. Some of these things will go into this blog. These thoughts are the thoughts of a person, who was raised and socialized in southern Germany but lives in Berlin since 2011. . Having learned my basic scientific education at Technische Universität Berlin (aerospace engineering), I realized early on how important it is for me to view my engineering activities in a broader sociocultural context. My time at HPI School of Design Thinking further sharpened my personal perspective on the (mostly) wonderful world we are living in. And as long as I can read Roger Willemsen’s books, I am grateful for each day on Earth. . Feel free to reach out on my on Twitter (@niphiwi or LinkedIn. . To learn something about my pasta, bread or croissants, please visit my Instagram (@brotbubi). .",
          "url": "https://niphiwi.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://niphiwi.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}